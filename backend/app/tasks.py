from celery_app import celery_app
from utils.config import load_config
from utils.datahandler import load_and_transform_data
from utils.pipelines import run_base_plus_pipeline, run_base_pipeline
from utils.common import generate_monthly_period, setup_custom_logging
from utils.db_usage import upload_pipeline_result_to_db, SHEET_TO_TABLE, set_pipeline_column, export_pipeline_tables_to_excel, process_exchange_rate_file
from utils.training_status import training_status_manager
from taxes.data_preparation import prepare_tax_data
from taxes.forecast import forecast_taxes
from taxes.db import init_db, get_all_forecast_pairs, restore_excel_from_db, save_excel_to_db
import os
from datetime import datetime
import shutil

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ –¥–ª—è Celery worker —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ª–æ–≥–æ–≤
os.environ['CELERY_WORKER_PROCESS'] = '1'

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –ª–æ–≥–æ–≤
log_dir = os.path.join(os.path.dirname(__file__), 'logs')
os.makedirs(log_dir, exist_ok=True)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è Celery worker
logger = setup_custom_logging(os.path.join(log_dir, "celery_worker.log"))

CONFIG_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), '../config_refined.yaml'))
RESULTS_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '../results'))
TRAINING_FILES_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), 'training_files'))

@celery_app.task(bind=True, track_started=True)
def train_task(self, pipeline, items_list, date, data_path, result_file_name):
    logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –∑–∞–¥–∞—á–∏ train_task: pipeline={pipeline}, items={len(items_list)}, date={date}")
    
    config = load_config(CONFIG_PATH)
    DATE_COLUMN = config['DATE_COLUMN']
    RATE_COLUMN = config['RATE_COLUMN']
    ITEM_ID = config['item_id']
    FACTOR = config['factor']
    models_to_use = config['models_to_use']
    TABPFNMIX_model = config.get('TABPFNMIX_model', {})
    METRIC = config['metric_for_training']
    ITEMS_TO_PREDICT = {key: config['–°—Ç–∞—Ç—å—è'][key] for key in items_list}
    CHOSEN_MONTH = datetime.strptime(date, "%Y-%m-%d")
    MONTHES_TO_PREDICT = generate_monthly_period(CHOSEN_MONTH)
    df_all_items = load_and_transform_data(data_path, DATE_COLUMN, RATE_COLUMN)
    
    logger.info(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(ITEMS_TO_PREDICT)} —Å—Ç–∞—Ç–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫—É—Ä—Å—ã –≤–∞–ª—é—Ç –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –¥–∞–Ω–Ω—ã—Ö
    try:
        if data_path and os.path.exists(data_path):
            logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—É—Ä—Å–æ–≤ –≤–∞–ª—é—Ç –∏–∑ —Ñ–∞–π–ª–∞ –¥–∞–Ω–Ω—ã—Ö: {data_path}")
            exchange_result = process_exchange_rate_file(data_path, '–ö—É—Ä—Å', '–î–∞—Ç–∞')
            logger.info(f"–ö—É—Ä—Å—ã –≤–∞–ª—é—Ç –æ–±–Ω–æ–≤–ª–µ–Ω—ã: {exchange_result['message']}")
        else:
            logger.warning(f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_path}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∫—É—Ä—Å–æ–≤ –≤–∞–ª—é—Ç: {e}")
        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞, –¥–∞–∂–µ –µ—Å–ª–∏ –∫—É—Ä—Å—ã –Ω–µ –æ–±–Ω–æ–≤–∏–ª–∏—Å—å
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç—É—Å–∞ –æ–±—É—á–µ–Ω–∏—è
    total_articles = len(ITEMS_TO_PREDICT)
    task_id = self.request.id
    training_status_manager.initialize_training(total_articles, task_id)
    logger.info(f"–°—Ç–∞—Ç—É—Å –æ–±—É—á–µ–Ω–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è {total_articles} —Å—Ç–∞—Ç–µ–π")
    
    # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ –ë–î –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ pipeline
    prev_path = os.path.join(TRAINING_FILES_DIR, f"prev_{datetime.now().strftime('%Y%m%d%H%M%S')}.xlsx")
    # –ü–µ—Ä–µ–¥–∞–µ–º pipeline ('base' –∏–ª–∏ 'base+') –≤ —Ñ—É–Ω–∫—Ü–∏—é —ç–∫—Å–ø–æ—Ä—Ç–∞
    pipeline_arg = None
    if pipeline == "BASE+":
        pipeline_arg = 'base+'
    elif pipeline == "BASE":
        pipeline_arg = 'base'
    output = export_pipeline_tables_to_excel(SHEET_TO_TABLE, pipeline=pipeline_arg)
    with open(prev_path, "wb") as f:
        f.write(output.getvalue())
    logger.info(f"–°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {prev_path}")
    
    try:
        if pipeline == "BASE+":
            logger.info("–ó–∞–ø—É—Å–∫ BASE+ –ø–∞–π–ø–ª–∞–π–Ω–∞")
            run_base_plus_pipeline(
                df_all_items=df_all_items,
                ITEMS_TO_PREDICT=ITEMS_TO_PREDICT,
                config=config,
                DATE_COLUMN=DATE_COLUMN,
                RATE_COLUMN=RATE_COLUMN,
                ITEM_ID=ITEM_ID,
                FACTOR=FACTOR,
                models_to_use=models_to_use,
                TABPFNMIX_model=TABPFNMIX_model,
                METRIC=METRIC,
                CHOSEN_MONTH=CHOSEN_MONTH,
                MONTHES_TO_PREDICT=MONTHES_TO_PREDICT,
                result_file_name=result_file_name,
                prev_predicts_file=prev_path,
                status_manager=training_status_manager
            )
            logger.info("BASE+ –ø–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–µ–Ω, –∑–∞–≥—Ä—É–∑–∫–∞ –≤ –ë–î")
            upload_pipeline_result_to_db(result_file_name, SHEET_TO_TABLE, date, DATE_COLUMN, pipeline='base+')
            logger.info("–î–∞–Ω–Ω—ã–µ BASE+ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ë–î")
            
        elif pipeline == "BASE":
            logger.info("–ó–∞–ø—É—Å–∫ BASE –ø–∞–π–ø–ª–∞–π–Ω–∞")
            run_base_pipeline(
                df_all_items=df_all_items,
                ITEMS_TO_PREDICT=ITEMS_TO_PREDICT,
                config=config,
                DATE_COLUMN=DATE_COLUMN,
                RATE_COLUMN=RATE_COLUMN,
                ITEM_ID=ITEM_ID,
                FACTOR=FACTOR,
                models_to_use=models_to_use,
                METRIC=METRIC,
                CHOSEN_MONTH=CHOSEN_MONTH,
                MONTHES_TO_PREDICT=MONTHES_TO_PREDICT,
                result_file_name=result_file_name,
                prev_predicts_file=prev_path,
                status_manager=training_status_manager
            )
            logger.info("BASE –ø–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–µ–Ω, –∑–∞–≥—Ä—É–∑–∫–∞ –≤ –ë–î")
            upload_pipeline_result_to_db(result_file_name, SHEET_TO_TABLE, date, DATE_COLUMN, pipeline='base')
            logger.info("–î–∞–Ω–Ω—ã–µ BASE –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ë–î")
        
        # –ü—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏:
        # 1. –û—á–∏—â–∞–µ–º current_article (–±–æ–ª—å—à–µ –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º)
        training_status_manager.update_current_article('')
        
        # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º –ì–õ–û–ë–ê–õ–¨–ù–´–ô —Å—Ç–∞—Ç—É—Å –¥–ª—è –í–°–ï–• –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        training_status_manager.save_completed_training({
            'status': 'done',
            'message': '–ü—Ä–æ–≥–Ω–æ–∑ –≥–æ—Ç–æ–≤. –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î –∏ –≥–æ—Ç–æ–≤—ã –∫ –≤—ã–≥—Ä—É–∑–∫–µ!',
            'pipeline': pipeline,
            'date': date,
            'articles': list(ITEMS_TO_PREDICT.keys()),  # –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–∞—Ç–µ–π –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–ø—É—â–µ–Ω–æ –æ–±—É—á–µ–Ω–∏–µ
            'completed_at': datetime.now().isoformat()
        })
        
        # 3. –í–ê–ñ–ù–û: –û—á–∏—â–∞–µ–º current_task_id - —Ä–∞–∑–±–ª–æ–∫–∏—Ä—É–µ–º –∑–∞–ø—É—Å–∫ –Ω–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        training_status_manager.redis_client.delete(training_status_manager.KEYS['current_task_id'])
        
        logger.info(f"‚úÖ –ó–∞–¥–∞—á–∞ train_task —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {result_file_name}")
        return {"status": "done"}
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∑–∞–¥–∞—á–µ train_task: {e}", exc_info=True)
        
        # –ü—Ä–∏ –æ—à–∏–±–∫–µ —Ç–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å
        training_status_manager.save_completed_training({
            'status': 'error',
            'message': f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {str(e)}',
            'error': str(e),
            'articles': list(ITEMS_TO_PREDICT.keys()),  # –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–∞—Ç–µ–π, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –±—ã–ª–∞ –ø–æ–ø—ã—Ç–∫–∞ –æ–±—É—á–µ–Ω–∏—è
            'completed_at': datetime.now().isoformat()
        })
        
        # –û—á–∏—â–∞–µ–º current_task_id –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å
        training_status_manager.redis_client.delete(training_status_manager.KEYS['current_task_id'])
        training_status_manager.clear_training_progress()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª–∞ –ª–∏ –∑–∞–¥–∞—á–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞
        try:
            from celery.exceptions import Revoked
            if isinstance(e, Revoked):
                logger.warning("–ó–∞–¥–∞—á–∞ –±—ã–ª–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞ (Revoked)")
                return {"status": "revoked", "error": "Task was revoked"}
        except ImportError:
            # Fallback –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ Celery
            if 'revoked' in str(e).lower():
                logger.warning("–ó–∞–¥–∞—á–∞ –±—ã–ª–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞ (revoked –≤ —Å—Ç—Ä–æ–∫–µ –æ—à–∏–±–∫–∏)")
                return {"status": "revoked", "error": "Task was revoked"}
        
        return {"status": "error", "error": str(e)}

@celery_app.task(bind=True)
def run_tax_forecast_task(self, history_file_path: str, forecast_date_str: str, selected_groups: dict):
    try:
        # 1. Init DB
        init_db()
        
        # 2. Prepare Data
        self.update_state(state='PROGRESS', meta={'status': 'Preparing data...'})
        # Ensure data directory exists as prepare_tax_data might need it or use it
        os.makedirs('data', exist_ok=True)
        prepare_tax_data(history_file_path)
        
        # 3. Restore previous forecasts from DB
        self.update_state(state='PROGRESS', meta={'status': 'Restoring previous forecasts...'})
        target_dir = os.path.join('results', '–†–∞–∑–Ω–∏—Ü–∞ –ê–∫—Ç–∏–≤–æ–≤ –∏ –ü–∞—Å—Å–∏–≤–æ–≤')
        os.makedirs(target_dir, exist_ok=True)
        
        forecast_pairs = get_all_forecast_pairs()
        for factor, item_id in forecast_pairs:
            file_data = restore_excel_from_db(factor, item_id)
            if file_data:
                # Reconstruct filename: {factor}_{item_id}_predict_BASE.xlsx
                filename = f"{factor}_{item_id}_predict_BASE.xlsx"
                with open(os.path.join(target_dir, filename), 'wb') as f:
                    f.write(file_data)
                    
        # 4. Run Forecast
        # forecast_date_str is expected to be YYYY-MM-DD
        forecast_date = datetime.strptime(forecast_date_str, "%Y-%m-%d")
        
        def progress_callback(msg):
            self.update_state(state='PROGRESS', meta={'status': f'Forecasting: {msg}'})
            
        forecast_taxes(forecast_date, selected_groups, progress_callback=progress_callback)
        
        # 5. Save updated forecasts to DB
        self.update_state(state='PROGRESS', meta={'status': 'Saving results to DB...'})
        for filename in os.listdir(target_dir):
            if filename.endswith('.xlsx') or filename.endswith('.xls'):
                file_path = os.path.join(target_dir, filename)
                with open(file_path, 'rb') as f:
                    save_excel_to_db(filename, f.read())
                    
        # 6. Zip results
        self.update_state(state='PROGRESS', meta={'status': 'Zipping results...'})
        # Create zip in a known location
        zip_name = f"forecast_results_{datetime.now().strftime('%Y%m%d%H%M%S')}"
        zip_path = shutil.make_archive(zip_name, 'zip', 'results')
        
        # Move zip to a static folder if needed, or just return the path
        # For now, return the absolute path
        abs_zip_path = os.path.abspath(zip_path)
        
        return {'status': 'Completed', 'zip_path': abs_zip_path}
        
    except Exception as e:
        logger.error(f"Task failed: {e}")
        self.update_state(state='FAILURE', meta={'error': str(e)})
        raise e
